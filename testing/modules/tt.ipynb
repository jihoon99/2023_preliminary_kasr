{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def data_processing(data, data_type=\"train\"):\n",
    "    spectrograms = []\n",
    "    labels = []\n",
    "    input_lengths = []\n",
    "    label_lengths = []\n",
    "    for (waveform, _, utterance, _, _, _) in data:\n",
    "        if data_type == 'train':\n",
    "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        elif data_type == 'valid':\n",
    "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        else:\n",
    "            raise Exception('data_type should be train or valid')\n",
    "        spectrograms.append(spec)\n",
    "        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
    "        labels.append(label)\n",
    "        input_lengths.append(spec.shape[0]//2)\n",
    "        label_lengths.append(len(label))\n",
    "\n",
    "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "\n",
    "    return spectrograms, labels, input_lengths, label_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 80, 120])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq1 = 100\n",
    "mel = 80\n",
    "\n",
    "import torch\n",
    "spec = torch.randn([mel, seq1]).transpose(0,1)\n",
    "spec1 = torch.randn([mel, 120]).transpose(0,1)\n",
    "spectrograms = [spec, spec1]\n",
    "\n",
    "import torch.nn as nn\n",
    "nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2,3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 120, 80])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2820, -0.0804,  0.7618],\n",
       "         [-0.1416,  0.9416, -0.7956],\n",
       "         [ 0.0633, -0.8903,  1.0790],\n",
       "         [-0.6923,  1.2591,  0.4260],\n",
       "         [-0.4973, -0.4562, -0.4418]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "template = nn.ZeroPad2d((0,3-5,0,0))\n",
    "\n",
    "aa = torch.randn([1,5,5])\n",
    "template(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.randint(100, [8,30])\n",
    "\n",
    "zero_targets = torch.zeros(8, 30).to(torch.long)\n",
    "\n",
    "for idx, target in enumerate(targets):\n",
    "    zero_targets[idx].narrow(0,0,len(target)).copy_(torch.LongTensor(target))\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[52, 68, 13, 25, 82, 32, 22, 19, 49,  8, 56,  9, 32, 25, 28, 54, 70, 11,\n",
       "         13, 32, 43, 97, 96, 82, 85, 23, 79, 36,  8, 81],\n",
       "        [47, 82,  5, 61, 18, 67, 55,  8, 40, 66, 20,  9, 76, 53, 67, 55,  9,  6,\n",
       "         54, 11, 50, 66, 79,  9, 29, 59, 66, 15, 82, 88],\n",
       "        [16, 36, 11, 28, 37, 33, 45,  3, 35, 80, 31,  8, 38, 36,  2, 74, 71, 37,\n",
       "         24, 81, 46,  5, 30, 48, 21, 57, 84, 68, 61, 99],\n",
       "        [37, 22, 58, 45, 47, 60, 91, 41, 97, 28, 89, 44, 30, 96, 12, 63, 93, 10,\n",
       "         87, 93, 68, 37, 80, 29, 35, 15, 27, 61, 67, 69],\n",
       "        [46, 88, 58, 53, 57,  9, 56,  8, 95, 30, 41, 70, 79, 18, 24,  4, 54, 72,\n",
       "         74, 16, 43, 44, 75, 44, 89, 50, 99, 18, 27, 59],\n",
       "        [ 3, 55,  1,  3, 24, 53, 26,  2,  6, 89, 32, 42, 81, 80, 68, 26, 94, 93,\n",
       "         56, 38, 66, 17, 42, 32, 48, 84, 89,  9, 48, 79],\n",
       "        [43, 87, 13, 27, 94, 65, 91, 42, 94, 42, 85, 20, 71, 20, 13, 43,  0, 39,\n",
       "         30, 45, 60, 23, 52, 78,  9, 91, 72, 95, 56, 11],\n",
       "        [ 3, 27,  2, 56, 66, 50, 13,  8, 98, 28, 68,  1, 17, 85, 79, 36, 49, 99,\n",
       "         68, 33, 29, 18, 84, 38,  6, 18, 68, 26, 39, 91]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[52, 68, 13, 25, 82, 32, 22, 19, 49,  8, 56,  9, 32, 25, 28, 54, 70, 11,\n",
       "         13, 32, 43, 97, 96, 82, 85, 23, 79, 36,  8, 81],\n",
       "        [47, 82,  5, 61, 18, 67, 55,  8, 40, 66, 20,  9, 76, 53, 67, 55,  9,  6,\n",
       "         54, 11, 50, 66, 79,  9, 29, 59, 66, 15, 82, 88],\n",
       "        [16, 36, 11, 28, 37, 33, 45,  3, 35, 80, 31,  8, 38, 36,  2, 74, 71, 37,\n",
       "         24, 81, 46,  5, 30, 48, 21, 57, 84, 68, 61, 99],\n",
       "        [37, 22, 58, 45, 47, 60, 91, 41, 97, 28, 89, 44, 30, 96, 12, 63, 93, 10,\n",
       "         87, 93, 68, 37, 80, 29, 35, 15, 27, 61, 67, 69],\n",
       "        [46, 88, 58, 53, 57,  9, 56,  8, 95, 30, 41, 70, 79, 18, 24,  4, 54, 72,\n",
       "         74, 16, 43, 44, 75, 44, 89, 50, 99, 18, 27, 59],\n",
       "        [ 3, 55,  1,  3, 24, 53, 26,  2,  6, 89, 32, 42, 81, 80, 68, 26, 94, 93,\n",
       "         56, 38, 66, 17, 42, 32, 48, 84, 89,  9, 48, 79],\n",
       "        [43, 87, 13, 27, 94, 65, 91, 42, 94, 42, 85, 20, 71, 20, 13, 43,  0, 39,\n",
       "         30, 45, 60, 23, 52, 78,  9, 91, 72, 95, 56, 11],\n",
       "        [ 3, 27,  2, 56, 66, 50, 13,  8, 98, 28, 68,  1, 17, 85, 79, 36, 49, 99,\n",
       "         68, 33, 29, 18, 84, 38,  6, 18, 68, 26, 39, 91]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2820, -0.0804,  0.7618, -0.5526, -1.1497],\n",
       "         [-0.1416,  0.9416, -0.7956, -0.7145, -0.3202],\n",
       "         [ 0.0633, -0.8903,  1.0790, -0.5745, -1.1206],\n",
       "         [-0.6923,  1.2591,  0.4260, -0.9567, -0.2630],\n",
       "         [-0.4973, -0.4562, -0.4418, -0.2685,  0.2842]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rainism/opt/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/preprocessing/_data.py:240: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\n",
      "/Users/rainism/opt/anaconda3/envs/torch/lib/python3.9/site-packages/sklearn/preprocessing/_data.py:259: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import sklearn\n",
    "import torch\n",
    "from noisereduce import reduce_noise\n",
    "import numpy as np\n",
    "\n",
    "def remove_noise_data(np_wav, ratio=16_000):\n",
    "    return reduce_noise(y = np_wav, sr=ratio, stationary=False)\n",
    "\n",
    "def detect_silence(pcm, audio_threshold = 0.0075, min_silence_len = 3, ratio=16000, make_silence_len=1):\n",
    "    if len(pcm) < min_silence_len*ratio:\n",
    "        return pcm\n",
    "    \n",
    "    b = np.where((abs(pcm) > audio_threshold) == True)[0] # 소리가 나는 부분\n",
    "    c = np.concatenate(([0], b[:-1]), axis=0)\n",
    "\n",
    "    starts = c[(b-c)>min_silence_len*ratio]               # 소리가 안나는 부분 시작\n",
    "    ends = b[(b-c)>min_silence_len*ratio]\n",
    "\n",
    "    if len(ends) == 0:\n",
    "        return pcm\n",
    "    else:\n",
    "        non_masking = np.array([True]*len(pcm))\n",
    "        for (s,e) in zip(starts, ends):\n",
    "            non_masking[s:e+1] = False\n",
    "            non_masking[e-make_silence_len*ratio:e+1] = True\n",
    "        \n",
    "        return pcm[non_masking]\n",
    "\n",
    "\n",
    "def wav2image_tensor(path):\n",
    "    audio, sr = librosa.load(path, sr=16_000)\n",
    "    audio, _ = librosa.effects.trim(audio)\n",
    "\n",
    "    audio = remove_noise_data(audio)\n",
    "\n",
    "    if True:\n",
    "        audio = detect_silence(\n",
    "            audio,\n",
    "            audio_threshold=0.0075,\n",
    "            min_silence_len=3,\n",
    "            ratio = 16_000,\n",
    "            make_silence_len=1\n",
    "            )\n",
    "\n",
    "    ######### 하드 코딩 된 부분들\n",
    "    mfcc = librosa.feature.mfcc(\n",
    "        y = audio, \n",
    "        sr=16_000, \n",
    "        n_mfcc=80, \n",
    "        n_fft=400, \n",
    "        hop_length=160\n",
    "    )\n",
    "\n",
    "    if True:\n",
    "        mfcc = remove_noise_data(mfcc)\n",
    "\n",
    "\n",
    "    ########################3 하드 코딩 부분 변경 ##################\n",
    "    # max_len = 1000\n",
    "    mfcc = sklearn.preprocessing.scale(mfcc, axis=1)\n",
    "    # def pad2d(a, i): return a[:, 0:i] if a.shape[1] > i else np.hstack(\n",
    "    #     (a, np.zeros((a.shape[0], i-a.shape[1]))))\n",
    "    # padded_mfcc = pad2d(mfcc, max_len).reshape( \n",
    "    #     1, self.config.n_mels, max_len)  # 채널 추가\n",
    "    mfcc = torch.tensor(mfcc, dtype=torch.float)\n",
    "\n",
    "    #######################################################   reshape을 해줘야 할 수 도있음.  deepspeech2인가 하는놈은 bs, feat, leng로 들어가는 것 같은데..\n",
    "\n",
    "    return mfcc\n",
    "\n",
    "path = '/Users/rainism/Desktop/2023_AI_hub/2023_preliminary_kasr/task2_03.wav'\n",
    "\n",
    "audio = wav2image_tensor(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 275])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.84535474, 0.885125)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove noise\n",
    "audio.min(), audio.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-694.2284, 245.41486)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# withoud scale, remove_noise\n",
    "mfcc.min(),mfcc.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4.6030335, 5.488033)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with sklearn scale, remove_noise\n",
    "mfcc.min(),mfcc.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-611.1071, 210.56094)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfcc.min(), mfcc.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4.1439986, 5.7186747)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfcc.min(), mfcc.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
